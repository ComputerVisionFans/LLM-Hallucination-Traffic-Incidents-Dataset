{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open(\"key.txt\",\"r\").read().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14516\\1241897912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m completion = openai.ChatCompletion.create(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-4\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"role\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"How can i use Chatgpt-api\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m             return (\n\u001b[1;32m--> 620\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    684\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"How can i use Chatgpt-api\"}]\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = completion['choices'][0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install matplotlib\n",
    "!pip install pydantic\n",
    "!pip install pyecharts\n",
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT GPT generated top station data\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from pyecharts.charts import Bar, Page, Pie, Grid\n",
    "from pyecharts import options as opts\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to plot the gpt data\n",
    "def plot_gpt_data_to_html(filtered_gpt_data):\n",
    "    data = filtered_gpt_data\n",
    "\n",
    "    # Updated pattern to handle both data formats\n",
    "    pattern = r'(\\w[\\w\\s,]*[^\\s])\\s\\((\\d+)\\)|\\(([\\w\\s,]+), (\\d+)\\)'\n",
    "    matches = re.findall(pattern, data)\n",
    "\n",
    "    # Extracting station names and frequencies considering both matching groups from the updated pattern\n",
    "    stations = [match[0] if match[2] == '' else match[2] for match in matches]\n",
    "    frequencies = [int(match[1]) if match[3] == '' else int(match[3]) for match in matches]\n",
    "\n",
    "    bar_chart = (\n",
    "        Bar()\n",
    "        .add_xaxis(stations)\n",
    "        .add_yaxis(\"Frequenz\", frequencies, tooltip_opts=opts.TooltipOpts(is_show=True, trigger=\"axis\", axis_pointer_type=\"cross\"))\n",
    "        .set_global_opts(\n",
    "            title_opts=opts.TitleOpts(title=\"Top 10 Betroffenen Stationen\"),  \n",
    "            xaxis_opts=opts.AxisOpts(\n",
    "                name=\"Stationen\",  # X-Axis title\n",
    "                axislabel_opts=opts.LabelOpts(\n",
    "                    rotate=45,   # Try 45-degree rotation\n",
    "                    font_size=8  # Further reduce font size\n",
    "                )\n",
    "            ),\n",
    "            yaxis_opts=opts.AxisOpts(\n",
    "                name=\"Frequenz\",  # Y-Axis title\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return bar_chart.render_notebook()\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to plot the line data\n",
    "def plot_line_data_to_html(lines_data_list):\n",
    "    data = lines_data_list\n",
    "    page = Page(layout=Page.SimplePageLayout)\n",
    "    bars = []\n",
    "\n",
    "    for idx, week_data in enumerate(data, 1):\n",
    "        labels, values = zip(*week_data)\n",
    "        bar_chart = (\n",
    "            Bar()\n",
    "            .add_xaxis(list(labels))\n",
    "            .add_yaxis(f\"Woche {idx}\", list(values), label_opts=opts.LabelOpts(is_show=False))\n",
    "            .set_global_opts(\n",
    "                title_opts=opts.TitleOpts(title=f\"Woche {idx} Vorfälle je Linie\"),\n",
    "                xaxis_opts=opts.AxisOpts(\n",
    "                    name=\"Linie\",  # X-Axis title\n",
    "                    axislabel_opts=opts.LabelOpts(rotate=45)  # Optional: Rotate x-axis labels if needed\n",
    "                ),\n",
    "                yaxis_opts=opts.AxisOpts(\n",
    "                    name=\"Anzahl Vorfälle\"  # Y-Axis title\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        bars.append(bar_chart)\n",
    "\n",
    "    page.add(*bars)\n",
    "    \n",
    "    print(\"Generating plot...\")\n",
    "    # Return the embeddable HTML\n",
    "    #return page.render_embed()\n",
    "    return page.render_notebook()\n",
    "\n",
    "# Define the function to plot the accident data\n",
    "def plot_accident_data_to_html(accident_data_list):\n",
    "    data = accident_data_list\n",
    "\n",
    "    def generate_pie(data_item, title):\n",
    "        categories = list(data_item.keys())\n",
    "        values = list(data_item.values())\n",
    "        pie = (\n",
    "            Pie()\n",
    "            .add(\"\", [list(z) for z in zip(categories, values)])\n",
    "            .set_global_opts(title_opts=opts.TitleOpts(title=title), legend_opts=opts.LegendOpts(orient=\"vertical\", pos_top=\"15%\", pos_left=\"85%\"))\n",
    "            .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}: {d}%\"))  # Displaying category name and percentage\n",
    "        )\n",
    "        return pie\n",
    "\n",
    "    page = Page()\n",
    "    for index, data_item in enumerate(data):\n",
    "        page.add(generate_pie(data_item, f\"Woche {index+1}\"))\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    # Return the embeddable HTML\n",
    "    #return page.render_embed()\n",
    "    return page.render_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accident_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.5,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.6,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.4,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.2,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.8,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0.9,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=1.0,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import os\n",
    "from pyecharts import *\n",
    "#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html \n",
    "\n",
    "load_dotenv()\n",
    "# Configure the OpenAI API client\n",
    "#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to generate the response\n",
    "#def generate_response(resulted_data):\n",
    "#    response = openai.ChatCompletion.create(\n",
    "#        model=\"gpt-4\",\n",
    "        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "#        temperature = 0.4,\n",
    "#        messages=[\n",
    "#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n",
    "#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "#            }\n",
    "#        ]\n",
    "#    )\n",
    "#    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_response(resulted_data):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0314\",\n",
    "    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Du bist ein Analyst. Aus den bereitgestellten Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-List-Daten:\" + resulted_data + \"Im Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Define the function to filter the accident data\n",
    "def get_incident_data(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    #gpt filter\n",
    "    filtered_incidents = [\n",
    "        {\n",
    "            'title': entry['title'],\n",
    "            'description': entry['description']\n",
    "        } \n",
    "        for entry in data['incidents'] \n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n",
    "    ]\n",
    "        \n",
    "    resulted_data = json.dumps(filtered_incidents)\n",
    "    \n",
    "    print(resulted_data)\n",
    "    \n",
    "    #filtered_gpt_data = ''\n",
    "    filtered_gpt_data = generate_response(resulted_data)\n",
    "    \n",
    "    return filtered_gpt_data\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to filter the line data \n",
    "def get_top10_lines(data, start_time, end_time):        \n",
    "    # Convert the provided timestamp range strings to datetime objects\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "    \n",
    "    filtered_lines_incidents = []\n",
    "    for entry in data['incidents']:\n",
    "        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime:\n",
    "            filtered_lines_incidents.append(entry['lines'])\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_list = [item for sublist in filtered_lines_incidents for item in sublist]\n",
    "    # Calculate the frequency of each element\n",
    "    element_frequencies = Counter(flattened_list)\n",
    "    # Get the top 10 most common elements and their frequencies\n",
    "    top_10 = element_frequencies.most_common(10)\n",
    "\n",
    "    return top_10\n",
    "\n",
    "# Define the function to filter the accident time data\n",
    "def get_accident_time_sum(data, start_time, end_time):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_datetime = datetime.strptime(start_time, date_format)\n",
    "    end_datetime = datetime.strptime(end_time, date_format)\n",
    "\n",
    "    filtered_accident_sum = {\"Verkehrsunfall\": 0, \"Falschparker\": 0}\n",
    "    #for entry in data['incidents']:\n",
    "    for entry in data['incidents']:\n",
    "        # Data validation for the 'start' and 'end' timestamps\n",
    "        if 'start' not in entry or 'end' not in entry:\n",
    "            #print(\"Missing 'start' or 'end' in entry:\", entry)\n",
    "            continue\n",
    "        if not entry['start'] or not entry['end']:\n",
    "            #print(\"Empty 'start' or 'end' timestamp in entry:\", entry)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_incident_datetime = datetime.strptime(entry['start'], date_format)\n",
    "            end_incident_datetime = datetime.strptime(entry['end'], date_format)\n",
    "        except ValueError as e:\n",
    "            #print(f\"Error parsing timestamp in entry {entry}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check the incident title and compute the time sum\n",
    "        if start_datetime <= start_incident_datetime <= end_datetime and (\"Verkehrsunfall\" in entry['title'] or \"Falschparker\" in entry['title']):\n",
    "            if \"Verkehrsunfall\" in entry['title']:\n",
    "                filtered_accident_sum['Verkehrsunfall'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "            if \"Falschparker\" in entry['title']:\n",
    "                filtered_accident_sum['Falschparker'] += (end_incident_datetime - start_incident_datetime).total_seconds() / 60\n",
    "\n",
    "    filtered_accident_sum = {key: (round(value, 2) if isinstance(value, float) else value) for key, value in filtered_accident_sum.items()}\n",
    "\n",
    "    return filtered_accident_sum\n",
    "\n",
    "\n",
    "# Define the function to generate the date ranges\n",
    "def get_date_ranges():\n",
    "    # Initialize the end date to today's date at 00:00:00\n",
    "    end_time = datetime.strptime('2018-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n",
    "    # Define the time interval (1 week = 7 days)\n",
    "    interval = timedelta(days=1)\n",
    "    date_ranges = []\n",
    "    # Generate the ranges\n",
    "    for _ in range(3):\n",
    "        start_time = end_time - interval\n",
    "        date_ranges.append((start_time, end_time))\n",
    "        end_time = start_time\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "# Define the function to generate the Subway_line data\n",
    "def get_line_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #top 10 line data\n",
    "    lines_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n",
    "        #print(\"Line data from data analysis:\")\n",
    "        lines_data_list.append(filetered_top_10_line_data)\n",
    "    print(lines_data_list)\n",
    "    return plot_line_data_to_html(lines_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the accident data\n",
    "def get_accident_data(date_ranges):\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    #accident time sum daata\n",
    "    accident_data_list = []\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n",
    "        #print(\"Filtered Accident time data\")\n",
    "        accident_data_list.append(filtered_accident_time_data)\n",
    "    print(accident_data_list)\n",
    "    return plot_accident_data_to_html(accident_data_list)\n",
    "\n",
    "\n",
    "# Define the function to generate the gpt data\n",
    "def get_gpt_data(date_ranges):\n",
    "    # Use the list of tuples in a for loop\n",
    "    data = read_data()\n",
    "    for start, end in date_ranges:\n",
    "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"Start: {start_str}, End: {end_str}\")\n",
    "        filtered_gpt_data = get_incident_data(data, start_str, end_str)\n",
    "        \n",
    "        print(\"Filtered GPT station data:\")\n",
    "        print(filtered_gpt_data)\n",
    "        break\n",
    "    return plot_gpt_data_to_html(filtered_gpt_data)\n",
    "\n",
    "# Define the function to read the data\n",
    "def read_data():\n",
    "    with open('hypotheise_1.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "#get_line_data(get_date_ranges())\n",
    "#get_accident_data(get_date_ranges())\n",
    "#get_gpt_data(get_date_ranges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-08-16 hypo-1 tem-0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_data(get_date_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
