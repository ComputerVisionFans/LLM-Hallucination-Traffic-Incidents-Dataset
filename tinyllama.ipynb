{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPi2Cu6a1xUZzIB11gDLOOn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vki25P8CCWl9","executionInfo":{"status":"ok","timestamp":1705087705214,"user_tz":-60,"elapsed":7410,"user":{"displayName":"Jonas Lee","userId":"05461148370337450006"}},"outputId":"ae64cb51-7502-469d-9e35-f5bc996f9265"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n","Collecting jinja2\n","  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2) (2.1.3)\n","Installing collected packages: jinja2\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.2\n","    Uninstalling Jinja2-3.1.2:\n","      Successfully uninstalled Jinja2-3.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jinja2-3.1.3\n"]}],"source":["!pip install --upgrade jinja2"]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-gzwKsRCeYv","outputId":"3ac88ac0-7976-4ff3-df7e-e7adf3e1b88e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-9gz72peq\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-9gz72peq\n","  Resolved https://github.com/huggingface/transformers.git to commit 2382706a1ca038099214664dfa40f2f06883072c\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import pipeline\n"],"metadata":{"id":"SKagV6QDCiE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio_client"],"metadata":{"id":"VyZVbKt7C8ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install jinja2==3.1.3"],"metadata":{"id":"EdS49JNPCmb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyecharts"],"metadata":{"id":"kSqqLD_qCpA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","#from dotenv import load_dotenv\n","from datetime import datetime, timedelta\n","#import openai\n","import os\n","from gradio_client import Client\n","from pyecharts import *\n","#from plot import plot_line_data_to_html, plot_accident_data_to_html, plot_gpt_data_to_html\n","#PLOT GPT generated top station data\n","import re\n","import matplotlib.pyplot as plt\n","from pyecharts.charts import Bar, Page, Pie, Grid\n","from pyecharts import options as opts\n","\n","\n","#load_dotenv()\n","\n","# Define the function to plot the gpt data\n","def plot_gpt_data_to_html(filtered_gpt_data):\n","    data = filtered_gpt_data\n","\n","    # Updated pattern to handle both data formats\n","    pattern = r'(\\w[\\w\\s,]*[^\\s])\\s\\((\\d+)\\)|\\(([\\w\\s,]+), (\\d+)\\)'\n","    matches = re.findall(pattern, data)\n","\n","    # Extracting station names and frequencies considering both matching groups from the updated pattern\n","    stations = [match[0] if match[2] == '' else match[2] for match in matches]\n","    frequencies = [int(match[1]) if match[3] == '' else int(match[3]) for match in matches]\n","\n","    bar_chart = (\n","        Bar()\n","        .add_xaxis(stations)\n","        .add_yaxis(\"Frequenz\", frequencies, tooltip_opts=opts.TooltipOpts(is_show=True, trigger=\"axis\", axis_pointer_type=\"cross\"))\n","        .set_global_opts(\n","            title_opts=opts.TitleOpts(title=\"Top 10 Betroffenen Stationen\"),\n","            xaxis_opts=opts.AxisOpts(\n","                name=\"Stationen\",  # X-Axis title\n","                axislabel_opts=opts.LabelOpts(\n","                    rotate=45,   # Try 45-degree rotation\n","                    font_size=8  # Further reduce font size\n","                )\n","            ),\n","            yaxis_opts=opts.AxisOpts(\n","                name=\"Frequenz\",  # Y-Axis title\n","            )\n","        )\n","    )\n","\n","    return bar_chart.render_notebook()\n","\n","# Configure the OpenAI API client\n","#openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n","#openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n","#openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n","#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","# Define the function to generate the response\n","#def generate_response(resulted_data):\n","#    response = openai.ChatCompletion.create(\n","#        model=\"gpt-4\",\n","        #engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n","#        temperature = 0.4,\n","#        messages=[\n","#            {\"role\": \"system\", \"content\": \"Du bist ein Analyse-Chatbot. Aus den bereitgestellten JSON-Daten antwortest du auf Nutzerfragen, um Statistiken basierend auf Benutzereingaben zu erstellen. Dies sind die Kontext-JSON-Daten:\"+ resulted_data},\n","#            {\"role\": \"user\", \"content\": \"Im JSON-Datenkontext der Wiener-Linie sind unter Titel betroffene Linien und unter 'Beschreibung' betroffene Stationen verzeichnet. Welche 10 Stationen sind am häufigsten betroffen? Geben Sie nur in diesem Format aus: (Stationsname, Gesamtzahl der Vorfälle). Zum Beispiel: (Rotkreuzplatz, 10).\"\n","#            }\n","#        ]\n","#    )\n","#    return response['choices'][0]['message']['content']\n","\n","def generate_response(resulted_data):\n","\n","\n","    response = openai.ChatCompletion.create(\n","    model=\"gpt-4-0314\",\n","    # engine=os.getenv(\"DEPLOYMENT_NAME\"), # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n","    temperature=0.7,\n","    messages=[\n","      {\"role\": \"user\",\n","       \"content\": \"You are an analyst. From the data provided, you answer user questions to create statistics based on user input. This is the context list data:\" + resulted_data + \"In the Vienna Line data context, affected lines are under title and under 'Description' lists affected stations. Which 10 stations are most frequently affected? Only output in this format: (station name, total number of incidents). For example: (Rotkreuzplatz, 10).\"\n","      }\n","    ]\n","  )\n","    return response['choices'][0]['message']['content']\n","\n","\n","\n","def generate_response_tinyllama(resulted_data):\n","    client = Client(\"https://tinyllama-tinyllama-chat.hf.space/--replicas/q15sq/\")\n","    result = client.predict(\"You are an analyst. From the data provided, you answer user questions to create statistics based on user input. This is the context list data:\" + resulted_data + \"In the Vienna Line data context, affected lines are under title and under 'Description' lists affected stations. Which 10 stations are most frequently affected? Only output in this format: (station name, total number of incidents). For example: (Rotkreuzplatz, 10).\",\t # str  in 'Message' Textbox component\n","                            api_name=\"/chat\")\n","\n","    return result\n","\n","def generate_response_tinyllama2(resulted_data, temp):\n","\n","    pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16)\n","    # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n","    messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are an analyst. From the data provided, you answer user questions to create statistics based on user input. This is the context list data:\" + resulted_data + \"In the Vienna Line data context, affected lines are under title and under 'Description' lists affected stations. Which 10 stations are most frequently affected? Only output in this format: (station name, total number of incidents). For example: (Rotkreuzplatz, 10).\"\n","    }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=temp, top_k=50, top_p=0.95)\n","\n","    #print(outputs[0][\"generated_text\"])\n","\n","    #client = Client(\"https://tinyllama-tinyllama-chat.hf.space/--replicas/q15sq/\")\n","    #result = client.predict(\"You are an analyst. From the data provided, you answer user questions to create statistics based on user input. This is the context list data:\" + resulted_data + \"In the Vienna Line data context, affected lines are under title and under 'Description' lists affected stations. Which 10 stations are most frequently affected? Only output in this format: (station name, total number of incidents). For example: (Rotkreuzplatz, 10).\",\t # str  in 'Message' Textbox component\n","    #                        api_name=\"/chat\")\n","\n","    return outputs[0][\"generated_text\"]\n","\n","\n","# Define the function to filter the accident data\n","def get_incident_data(data, start_time, end_time, temp):\n","    # Convert the provided timestamp range strings to datetime objects\n","    date_format = \"%Y-%m-%d %H:%M:%S\"\n","    start_datetime = datetime.strptime(start_time, date_format)\n","    end_datetime = datetime.strptime(end_time, date_format)\n","\n","    #gpt filter\n","    filtered_incidents = [\n","        {\n","            'title': entry['title'],\n","            'description': entry['description']\n","        }\n","        for entry in data['incidents']\n","        if start_datetime <= datetime.strptime(entry['start'], date_format) <= end_datetime\n","    ]\n","\n","    resulted_data = json.dumps(filtered_incidents)\n","\n","    print(resulted_data)\n","\n","    #filtered_gpt_data = ''\n","    filtered_gpt_data = generate_response_tinyllama2(resulted_data, temp)\n","\n","    return filtered_gpt_data\n","\n","# Define the function to generate the date ranges\n","def get_date_ranges():\n","    # Initialize the end date to today's date at 00:00:00\n","    end_time = datetime.strptime('2014-08-16 00:00:00.00000','%Y-%m-%d %H:%M:%S.%f')\n","    # Define the time interval (1 week = 7 days)\n","    interval = timedelta(days=1)\n","    date_ranges = []\n","    # Generate the ranges\n","    for _ in range(3):\n","        start_time = end_time - interval\n","        date_ranges.append((start_time, end_time))\n","        end_time = start_time\n","    return date_ranges\n","\n","\n","# Define the function to generate the Subway_line data\n","def get_line_data(date_ranges):\n","    with open('original_2014-8-15.json', 'r') as file:\n","        data = json.load(file)\n","\n","    #top 10 line data\n","    lines_data_list = []\n","    for start, end in date_ranges:\n","        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n","        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n","        filetered_top_10_line_data = get_top10_lines(data, start_str, end_str)\n","        #print(\"Line data from data analysis:\")\n","        lines_data_list.append(filetered_top_10_line_data)\n","    print(lines_data_list)\n","    return plot_line_data_to_html(lines_data_list)\n","\n","\n","# Define the function to generate the accident data\n","def get_accident_data(date_ranges):\n","    with open('original_2014-8-15.json', 'r') as file:\n","        data = json.load(file)\n","\n","    #accident time sum daata\n","    accident_data_list = []\n","    for start, end in date_ranges:\n","        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n","        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n","        filtered_accident_time_data = get_accident_time_sum(data, start_str, end_str)\n","        #print(\"Filtered Accident time data\")\n","        accident_data_list.append(filtered_accident_time_data)\n","    print(accident_data_list)\n","    return plot_accident_data_to_html(accident_data_list)\n","\n","\n","# Define the function to generate the gpt data\n","def get_gpt_data(date_ranges, temp):\n","    # Use the list of tuples in a for loop\n","    data = read_data()\n","    for start, end in date_ranges:\n","        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n","        end_str = end.strftime('%Y-%m-%d %H:%M:%S')\n","\n","        print(f\"Start: {start_str}, End: {end_str}\")\n","        filtered_gpt_data = get_incident_data(data, start_str, end_str, temp)\n","\n","        print(\"Filtered TinyLLAM station data:\")\n","        print(filtered_gpt_data)\n","        break\n","    return plot_gpt_data_to_html(filtered_gpt_data)\n","\n","# Define the function to read the data\n","def read_data():\n","    with open('original_2014-8-15.json', 'r') as file:\n","        data = json.load(file)\n","    return data\n","#get_line_data(get_date_ranges())\n","#get_accident_data(get_date_ranges())\n","#get_gpt_data(get_date_ranges())\n"],"metadata":{"id":"08_zePpkCrcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.7)"],"metadata":{"id":"2NDCSsgIC059"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.5)"],"metadata":{"id":"f3kPxhjtJFy5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.4)"],"metadata":{"id":"uPQ2qKnrJHcz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.3)"],"metadata":{"id":"x5GqISZiJJES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.2)"],"metadata":{"id":"EKovJ-OHJKoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.1)"],"metadata":{"id":"ZpF5Hxl5JMHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.8)"],"metadata":{"id":"bEhDVD1eJNvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_gpt_data(get_date_ranges(),0.9)"],"metadata":{"id":"vl1k8RqdJPRk"},"execution_count":null,"outputs":[]}]}